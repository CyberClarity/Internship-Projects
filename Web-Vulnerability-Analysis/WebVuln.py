# web_vuln_scanner_cli.py
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time
import sys

# Test payloads for SQLi and XSS
SQLI_PAYLOADS = ["' OR '1'='1", "'--", "' OR '1'='1' --", "admin' --"]
XSS_PAYLOADS = ["<script>alert(1)</script>", "<img src=x onerror=alert(1)>", "<svg onload=alert(1)>"]
HEADERS = {'User-Agent': 'Mozilla/5.0'}

# Store visited URLs to avoid re-scanning
discovered_links = set()


def is_valid_url(url):
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)


def get_all_forms(url):
    try:
        soup = BeautifulSoup(requests.get(url, headers=HEADERS).content, "html.parser")
        return soup.find_all("form")
    except Exception as e:
        print(f"[!] Failed to fetch forms from {url}: {e}")
        return []


def get_all_links(url):
    links = set()
    try:
        soup = BeautifulSoup(requests.get(url, headers=HEADERS).content, "html.parser")
        for a_tag in soup.find_all("a"):
            href = a_tag.get("href")
            if href and href.startswith("/"):
                href = urljoin(url, href)
            if is_valid_url(href):
                links.add(href)
    except Exception as e:
        print(f"[!] Failed to fetch links from {url}: {e}")
    return links


def submit_form(form, url, value):
    try:
        action = form.get("action")
        method = form.get("method", "get").lower()
        post_url = urljoin(url, action)
        inputs = form.find_all("input")
        data = {}
        for input_tag in inputs:
            name = input_tag.get("name")
            if not name:
                continue
            data[name] = value

        if method == "post":
            return requests.post(post_url, data=data, headers=HEADERS)
        else:
            return requests.get(post_url, params=data, headers=HEADERS)
    except Exception as e:
        print(f"[!] Failed to submit form: {e}")
        return None


def test_xss(url):
    print(f"[*] Testing XSS on {url}")
    forms = get_all_forms(url)
    for form in forms:
        for payload in XSS_PAYLOADS:
            res = submit_form(form, url, payload)
            if res and payload in res.text:
                print(f"[+] XSS vulnerability detected on {url} with payload: {payload}")
                break


def test_sqli(url):
    print(f"[*] Testing SQL Injection on {url}")
    for payload in SQLI_PAYLOADS:
        try:
            new_url = f"{url}?q={payload}"
            res = requests.get(new_url, headers=HEADERS)
            if res and re.search("(SQL syntax|mysql_fetch|ORA-|unterminated string)", res.text, re.IGNORECASE):
                print(f"[+] SQL Injection vulnerability detected: {new_url}")
                break
        except Exception as e:
            print(f"[!] Error testing SQLi: {e}")


def crawl_and_scan(start_url):
    urls_to_scan = [start_url]

    while urls_to_scan:
        current_url = urls_to_scan.pop()
        if current_url in discovered_links:
            continue

        print(f"[~] Scanning: {current_url}")
        discovered_links.add(current_url)

        test_xss(current_url)
        test_sqli(current_url)

        new_links = get_all_links(current_url)
        for link in new_links:
            if link not in discovered_links:
                urls_to_scan.append(link)


if __name__ == "__main__":
    # Default URL fallback for non-interactive environments
    target = "http://testphp.vulnweb.com"
    if len(sys.argv) >= 2:
        target = sys.argv[1].strip()
        if not target.startswith("http"):
            target = "http://" + target
    else:
        print(f"[!] No URL provided. Using default target: {target}")

    print("\n[*] Starting scan...\n")
    crawl_and_scan(target)
    print("\n[âœ“] Scan complete.")
